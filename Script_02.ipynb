{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415af151-8a51-489b-9175-2c7c5a80b563",
   "metadata": {},
   "source": [
    "# funcion para obtener informacion sin limite de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f9f1c8-5dab-430c-8ed1-0e232ed4593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Configuración warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def elasticScroll(elasticParameters, query, pages):\n",
    "    # parametros de salida\n",
    "    # parametros del indice\n",
    "    elasticURL = elasticParameters[\"elasticURL\"]\n",
    "    elasticIndex = elasticParameters[\"elasticIndex\"]\n",
    "    elasticUser = elasticParameters[\"elasticUser\"]\n",
    "    elasticPassword = elasticParameters[\"elasticPassword\"]\n",
    "    \n",
    "    if(len(elasticURL)==0) or (len(elasticIndex)==0) or (len(elasticUser)==0) or (len(elasticPassword)==0):\n",
    "        raise Exception(\"Revisa los parametros\")\n",
    "    # se define la url que apunta al indice de elastic\n",
    "    url_search = f\"{elasticURL}/{elasticIndex}/_search?scroll=1m\"\n",
    "    # se ejecuta la query\n",
    "    response = requests.get(url_search, json=query, auth=HTTPBasicAuth(elasticUser, elasticPassword))\n",
    "    # retorna una lista con el resultado de la query\n",
    "    search = json.loads(response.text)\n",
    "    # guardamos el scroll id correspondiente a la query\n",
    "    scroll_id = search[\"_scroll_id\"]\n",
    "    # url scroll\n",
    "    url_scroll = f\"{elasticURL}/_search/scroll\"\n",
    "    scroll_query = {\n",
    "                \"scroll\": \"1m\",\n",
    "                \"scroll_id\": f\"{scroll_id}\"\n",
    "            }\n",
    "\n",
    "    # condiciones iniciales\n",
    "    scroll_search = {\"hits\":{\"hits\":1}}\n",
    "    if pages:\n",
    "        # hay paginacion\n",
    "        # condiciones iniciales\n",
    "        from_ = pages[\"from\"]\n",
    "        size_ = pages[\"size\"]\n",
    "        count = len(search[\"hits\"][\"hits\"])\n",
    "\n",
    "        while scroll_search[\"hits\"][\"hits\"] and count < from_ + size_:\n",
    "            scroll_response = requests.get(url_scroll, json=scroll_query, auth=HTTPBasicAuth(elasticUser, elasticPassword))\n",
    "            scroll_search = json.loads(scroll_response.text)\n",
    "            if not scroll_search[\"hits\"][\"hits\"]:\n",
    "                continue\n",
    "            else:\n",
    "                search[\"hits\"][\"hits\"].extend(scroll_search[\"hits\"][\"hits\"])\n",
    "            count += len(scroll_search[\"hits\"][\"hits\"])\n",
    "\n",
    "        search[\"hits\"][\"hits\"] = search[\"hits\"][\"hits\"][from_:from_+size_+1]\n",
    "\n",
    "    else:\n",
    "        # Se devuelven todos los resultados\n",
    "        while scroll_search[\"hits\"][\"hits\"]:\n",
    "            scroll_response = requests.get(url_scroll, json=scroll_query,\n",
    "                                           auth=HTTPBasicAuth(elasticUser, elasticPassword))\n",
    "            scroll_search = json.loads(scroll_response.text)\n",
    "            if not scroll_search[\"hits\"][\"hits\"]:\n",
    "                continue\n",
    "            else:\n",
    "                search[\"hits\"][\"hits\"].extend(scroll_search[\"hits\"][\"hits\"])\n",
    "    # Se elina el campo scroll del scroll_body\n",
    "    del scroll_query[\"scroll\"]\n",
    "    # Se elimina el scroll de elasticsearch para liberar memoria\n",
    "    delete = requests.delete(url_scroll, json=scroll_query, auth=HTTPBasicAuth(elasticUser, elasticPassword))\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3328f701-9f28-4152-a314-149c155ba63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo parametros de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5c0705-d3d1-4688-9bee-ddacece2654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos de conexion a elastic\n",
    "elasticParameters = {\"elasticURL\": \"https://es-dev.e-contact.cl\"\n",
    "                    , \"elasticIndex\": \"lea_sequences-events-banco_de_chile\" \n",
    "                    , \"elasticUser\": \"jcalderon\"\n",
    "                    , \"elasticPassword\": \"jcalderon123\"\n",
    "                    }\n",
    "\n",
    "# query custom para obtener fechas\n",
    "query = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [],\n",
    "      \"filter\": [\n",
    "        {\n",
    "          \"match_all\": {}\n",
    "        },\n",
    "        {\n",
    "            \n",
    "          \"range\": {\n",
    "            \"interactionData.dateTimeUTC\": {\n",
    "              \"format\": \"strict_date_optional_time\",\n",
    "              \"gte\": \"2022-05-26T00:00:00.000Z\",\n",
    "              \"lte\": \"2022-05-30T00:00:00.000Z\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"should\": [],\n",
    "      \"must_not\": []\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# total de paginas\n",
    "pages =  {\n",
    "    \"from\": 0,\n",
    "    \"size\": 5000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dc6ea-9045-4d51-a314-d75fd083fbe6",
   "metadata": {},
   "source": [
    "# consulta a indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c685ff-eb24-4093-9a74-35903a2615dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = elasticScroll(elasticParameters, query, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a211b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    Función para realizar la limpieza de un texto dado.\n",
    "    \"\"\"\n",
    "    # Eliminamos los caracteres especiales\n",
    "   # texto = re.sub(r'\\W', ' ', texto)\n",
    "    # Eliminado las palabras que tengo un solo caracter\n",
    "    texto = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', str(texto))\n",
    "    # Sustituir los espacios en blanco en uno solo\n",
    "    texto = re.sub(r'\\s+', ' ', texto, flags=re.I)\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),(\"é\", \"e\"),(\"í\", \"i\"),(\"ó\", \"o\"),(\"ú\", \"u\"),\n",
    "        (\"ñ\", \"n\"),(\"?\", \"\"),(\"¿\", \"\"),(\"%\", \"\"),(\"$\", \"\"),\n",
    "        (\"#\", \"\"),(\"&\", \"\"),(\"(\", \"\"),(\")\", \"\"),(\"=\", \"\"),\n",
    "        (\"¡\", \"\"),(\"!\", \"\"),(\"*\", \"\"),(\"+\", \"\"),(\"~\", \"\"),\n",
    "        (\"[\", \"\"),(\"]\", \"\"),(\"}\", \"\"),(\"{\", \"\"),(\"^\", \"\"),\n",
    "        (\"<\", \"\"),(\">\", \"\"),(\"¬\", \"\"),(\"¨\", \"\"),(\"_\", \"\")\n",
    "    )\n",
    "    \n",
    "    for a, b in replacements:\n",
    "        texto = texto.replace(a, b)\n",
    "    # Convertimos textos a minusculas\n",
    "    #texto = texto.lower()\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a0418-36f5-4a96-bfad-345ae53c8550",
   "metadata": {},
   "source": [
    "# transformar response (dict) a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, json_normalize\n",
    "import numpy as np\n",
    "\n",
    "df = json_normalize(json.loads(json.dumps(response))[\"hits\"][\"hits\"])\n",
    "df_00 = df [['_source.conversationSequence.topicName','_source.interactionData.interactionId','_source.interactionData.dateTimeUTC']]\n",
    "df_00['topicName']= df_00['_source.conversationSequence.topicName']\n",
    "df_00['interactionId']= df_00['_source.interactionData.interactionId']\n",
    "df_00['Fec']= df_00['_source.interactionData.dateTimeUTC']\n",
    "df_00['hora']= pd.to_datetime(df_00['Fec'].apply(str).str.slice(start = 0, stop = 13))\n",
    "df_00['fecha'] = pd.to_datetime(df_00['Fec']).dt.date\n",
    "df_00['hora'] = pd.to_datetime(df_00['hora']).dt.time\n",
    "df_00['dia'] = pd.to_datetime(df_00['fecha'])\n",
    "df_00['dia'] = df_00['dia'].dt.day_name()\n",
    "df_00[\"topicName\"] = df_00[\"topicName\"].replace({ \"\": \"NO_IDENTIFICADO\", \" \": \"NO_IDENTIFICADO\" })\n",
    "df_00 = df_00[['topicName','interactionId','fecha','hora','dia']]\n",
    "#df_00['key'] =df_00['topicName'] + ',' + df_00['interactionId'] + ',' + str(df_00['fecha']) + ',' + str(df_00['hora'])+ ',' + df_00['dia']\n",
    "df_00 = df_00.drop_duplicates()\n",
    "\n",
    "df_grafo2 = pd.DataFrame({'topicName': ['Corte llamado', 'Red sucursal','Reclamación','Escalamiento de llamada','Reincidencia'], 'matched': True})\n",
    "\n",
    "# crea df 01 agrupando y contando por columna interactionId\n",
    "df_01=df_00.groupby([\"interactionId\"]).size().reset_index(name='cantidad')\n",
    "\n",
    "# concatena ambos df (00 y 01) para crear df 02\n",
    "df_02 = df_00.merge(df_01, how='inner', on='interactionId')\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "# filtro para aplicar en el df 02 cuando el campo cantidad == 1\n",
    "filter01 = df_02[\"cantidad\"]==1\n",
    "df_02.where(filter01, inplace = True)\n",
    "\n",
    "# crea df 03 apartando aquellos registros Nan\n",
    "df_03 = df_02[df_02['cantidad'].notna()]\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# crea df 04 agrupando y contando por columna interactionId\n",
    "df_04=df_00.groupby([\"interactionId\"]).size().reset_index(name='cantidad')\n",
    "\n",
    "# concatena ambos df (00 y 04) para crear df 05\n",
    "df_05 = df_00.merge(df_04, how='inner', on='interactionId')\n",
    "\n",
    "# filtro para aplicar en el df 05 cuando el campo cantidad != 1\n",
    "filter02 = df_05[\"cantidad\"]!=1\n",
    "df_05.where(filter02, inplace = True)\n",
    "\n",
    "# crea df 06 apartando aquellos registros Nan\n",
    "df_06 = df_05[df_05['cantidad'].notna()]\n",
    "\n",
    "# crea df  apartando aquellos registros cuyo topicName == 'NO_IDENTIFICADO'\n",
    "new_df = df_06[~df_06['topicName'].str.contains(\"NO_IDENTIFICADO\")]\n",
    "\n",
    "# concatena ambos df (03 y new_df) para crear df 01\n",
    "df_00 = pd.concat([df_03, new_df], axis=0)\n",
    "\n",
    "# elimina col cantidad\n",
    "del df_00['cantidad']\n",
    "\n",
    "df_00['contador']=1\n",
    "\n",
    "df_00['topicName'] = df_00.topicName.apply(limpiar_texto)\n",
    "\n",
    "############################ AGRUPA SEGUN REQUERIMIENTO (FECHA),(FECHA,HORA),(FECHA,HORA,DIA) ############################\n",
    "#agrupadoFecha = df_00.groupby(['topicName','fecha']).agg({'contador': 'count'}).reset_index()\n",
    "#agrupadoFecha['cantidad'] = agrupadoFecha['contador']\n",
    "#del agrupadoFecha['contador']\n",
    "#agrupadoFecha['total'] = agrupadoFecha['cantidad'].sum()\n",
    "#agrupadoFecha = df_grafo2.merge(agrupadoFecha, how='right', on='topicName')\n",
    "#agrupadoFecha['grafo']=agrupadoFecha['matched']\n",
    "#del agrupadoFecha['matched']\n",
    "#agrupadoFecha['grafo']=agrupadoFecha['grafo'].apply(lambda x: 1 if x!=1 else 0)\n",
    "#agrupadoFecha['grafo']=agrupadoFecha['grafo'].replace({0:2})\n",
    "\n",
    "#agrupadoFecha.to_csv(\"C:\\\\Users\\\\jcalderon\\\\Desktop\\\\Trafico\\\\Python_2021\\\\ProyectoBCH\\\\\" + 'Total'+'.csv', index = False)\n",
    "\n",
    "#agrupadoFechaHora = df_00.groupby(['topicName','fecha','hora']).agg({'contador': 'count'}).reset_index()\n",
    "#agrupadoFechaHora['cantidad'] = agrupadoFechaHora['contador']\n",
    "#del agrupadoFechaHora['contador']\n",
    "#agrupadoFechaHora_A = agrupadoFechaHora.groupby(['fecha','hora']).agg({'cantidad': 'sum'}).reset_index()\n",
    "#agrupadoFechaHora_A['total'] = agrupadoFechaHora_A['cantidad']\n",
    "#del agrupadoFechaHora_A['cantidad']\n",
    "#agrupadoFechaHora = agrupadoFechaHora.merge(agrupadoFechaHora_A, how='inner', on=['fecha','hora'])\n",
    "#agrupadoFechaHora['final'] =((agrupadoFechaHora['cantidad']) / agrupadoFechaHora['total'])\n",
    "#agrupadoFechaHora = df_grafo2.merge(agrupadoFechaHora, how='right', on='topicName')\n",
    "#agrupadoFechaHora['grafo']=agrupadoFechaHora['matched']\n",
    "#del agrupadoFechaHora['matched']\n",
    "#agrupadoFechaHora['grafo']=agrupadoFechaHora['grafo'].apply(lambda x: 1 if x!=1 else 0)\n",
    "#agrupadoFechaHora['grafo']=agrupadoFechaHora['grafo'].replace({0:2})\n",
    "\n",
    "#agrupadoFechaHora.to_csv(\"C:\\\\Users\\\\jcalderon\\\\Desktop\\\\Trafico\\\\Python_2021\\\\ProyectoBCH\\\\\" + 'Total'+'.csv', index = False)\n",
    "\n",
    "agrupadoFechaHoraDia = df_00.groupby(['topicName','fecha','hora','dia']).agg({'contador': 'count'}).reset_index()\n",
    "agrupadoFechaHoraDia['cantidad'] = agrupadoFechaHoraDia['contador']\n",
    "del agrupadoFechaHoraDia['contador']\n",
    "agrupadoFechaHoraDia_A = agrupadoFechaHoraDia.groupby(['fecha','hora','dia']).agg({'cantidad': 'sum'}).reset_index()\n",
    "agrupadoFechaHoraDia_A['total'] = agrupadoFechaHoraDia_A['cantidad']\n",
    "del agrupadoFechaHoraDia_A['cantidad']\n",
    "agrupadoFechaHoraDia = agrupadoFechaHoraDia.merge(agrupadoFechaHoraDia_A, how='inner', on=['fecha','hora','dia'])\n",
    "agrupadoFechaHoraDia['final'] =((agrupadoFechaHoraDia['cantidad']) / agrupadoFechaHoraDia['total'])\n",
    "agrupadoFechaHoraDia = df_grafo2.merge(agrupadoFechaHoraDia, how='right', on='topicName')\n",
    "agrupadoFechaHoraDia['grafo']=agrupadoFechaHoraDia['matched']\n",
    "del agrupadoFechaHoraDia['matched']\n",
    "agrupadoFechaHoraDia['grafo']=agrupadoFechaHoraDia['grafo'].apply(lambda x: 1 if x!=1 else 0)\n",
    "agrupadoFechaHoraDia['grafo']=agrupadoFechaHoraDia['grafo'].replace({0:2})\n",
    "\n",
    "\n",
    "agrupadoFechaHoraDia.to_csv(\"C:\\\\Users\\\\jcalderon\\\\Desktop\\\\Trafico\\\\Python_2021\\\\ProyectoBCH\\\\\" + 'Total'+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b8e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671bb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
